{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae9a56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "589ac071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasett\n",
    "raw_train_data = pd.read_csv('./SL Dataset/sign_mnist_train/sign_mnist_train.csv', sep=\",\")\n",
    "raw_test_data = pd.read_csv('./SL Dataset/sign_mnist_test/sign_mnist_test.csv', sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2edd733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 785)\n",
      "(7172, 785)\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_data.shape)\n",
    "print(raw_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8be9364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D numpy arrays\n",
    "def Data2NParray(train_data, test_data):\n",
    "    train = train_data.copy(deep=True)\n",
    "    test = test_data.copy(deep=True)\n",
    "    train_img = train.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "    test_img = test.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "    return train_img, test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33b540e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, test_img = Data2NParray(raw_train_data, raw_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "934e6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = raw_train_data['label'].values\n",
    "test_labels = raw_test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5fcee130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resape the images to a 3d array 1x28x28\n",
    "train_img = train_img.reshape(train_img.shape[0], 1, 28, 28)\n",
    "test_img = test_img.reshape(test_img.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3e3fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to tensors\n",
    "train_img_tensor = torch.from_numpy(train_img)\n",
    "train_label_tensor = torch.from_numpy(train_labels)\n",
    "\n",
    "test_img_tensor = torch.from_numpy(test_img)\n",
    "test_label_tensor = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d844815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch Dataset\n",
    "training_Dataset = TensorDataset(train_img_tensor, train_label_tensor)\n",
    "test_Dataset = TensorDataset(test_img_tensor, test_label_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9b42669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27455 torch.Size([1, 28, 28]) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# Each image is now converted to a (1,28,28) tensor\n",
    "image, label = training_Dataset[0]\n",
    "print(len(training_Dataset), image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11442c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7455, 20000, 7172)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Validation set and Training Set\n",
    "validation_sz = 7455\n",
    "training_sz = len(training_Dataset) - validation_sz\n",
    "\n",
    "training_set, validation_set = random_split(training_Dataset, [training_sz, validation_sz])\n",
    "\n",
    "len(validation_set), len(training_set), len(test_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "655826b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparmeters\n",
    "batch_size = 35\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "opt_func = torch.optim.SGD\n",
    "\n",
    "# Set constants for training the model\n",
    "input_ch = 1\n",
    "input_size = input_ch * 28 * 28\n",
    "classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b2e8152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e7016bc50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 16\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1373f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation\n",
    "train_dl = DataLoader(training_set, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "validation_dl = DataLoader(validation_set, batch_size*2, shuffle = True, num_workers=2, pin_memory=True)\n",
    "test_dl = DataLoader(test_Dataset, batch_size*2, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bdb91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Network class and make helper methods for training and validation\n",
    "class Network(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        # Generate Predictions\n",
    "        out = self(images)\n",
    "        # Calculate Loss\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        # Generate Predictions\n",
    "        out = self(images)\n",
    "        # Calculate Loss\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        # Calculate Accuracy\n",
    "        acc = accuracy(out, labels)\n",
    "        return{'val_loss': loss.detach(),'val_accuracy': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        # Combine Losses\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_acc = [x['val_accuracy'] for x in outputs]\n",
    "        # Combine Accuracies\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_accuracy': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train loss: {:.3f}, val loss {:.3f}, val acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_accuracy']))\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "805a254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(Network):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 28, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(28, 28, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # Ouput image size: 28x14x14\n",
    "\n",
    "            nn.Conv2d(28, 56, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(56, 56, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # Ouput image size: 56x7x7\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(56*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "064168e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=2744, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=128, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel(input_ch, classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e7f75efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    # Move Tensors to a chosen device\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    # Move Data to the device\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "            \n",
    "    def __len(self):\n",
    "        # Number of batches\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c374d0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ccd664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = to_device(CNNModel(input_ch, classes), device)\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(validation_dl, device)\n",
    "test_dl = DeviceDataLoader(test_dl, device)\n",
    "\n",
    "print(train_dl.device)\n",
    "print(test_dl.device)\n",
    "print(val_dl.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "273e7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_dl):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ab0aa250",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (70x400 and 2744x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [117]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [116]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_dl)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, val_dl):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mvalidation_step(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dl]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mvalidation_epoch_end(outputs)\n",
      "Input \u001b[0;32mIn [116]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, val_dl):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dl]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mvalidation_epoch_end(outputs)\n",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36mNetwork.validation_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate Predictions\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate Loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels)\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (70x400 and 2744x512)"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4e454bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train loss: 0.007, val loss 0.006, val acc: 0.9999\n",
      "Epoch [1], train loss: 0.004, val loss 0.004, val acc: 0.9997\n",
      "Epoch [2], train loss: 0.004, val loss 0.004, val acc: 0.9999\n",
      "Epoch [3], train loss: 0.002, val loss 0.003, val acc: 0.9997\n",
      "Epoch [4], train loss: 0.002, val loss 0.003, val acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, learning_rate , model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2400801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.5233004689216614, 'val_accuracy': 0.8751214742660522}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with the tetsing dataloader\n",
    "result = evaluate(model, test_dl)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff4aa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5478988e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f2d50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "869df177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(6) , Predicted: 6\n",
      "Label: tensor(5) , Predicted: 5\n",
      "Label: tensor(10) , Predicted: 10\n",
      "Label: tensor(0) , Predicted: 0\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(21) , Predicted: 21\n",
      "Label: tensor(10) , Predicted: 17\n",
      "Label: tensor(14) , Predicted: 14\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(7) , Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    img, label = test_Dataset[x]\n",
    "    print('Label:', label, ', Predicted:', predict_image(img, model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "740d1f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %network.0.weight : Float(28, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.0.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.2.weight : Float(28, 28, 3, 3, strides=[252, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.2.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.5.weight : Float(56, 28, 3, 3, strides=[252, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.5.bias : Float(56, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.7.weight : Float(56, 56, 3, 3, strides=[504, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.7.bias : Float(56, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.11.weight : Float(512, 2744, strides=[2744, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.11.bias : Float(512, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.13.weight : Float(128, 512, strides=[512, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.13.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.15.weight : Float(26, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.15.bias : Float(26, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %15 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.1, %network.0.weight, %network.0.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %16 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%15) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %17 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%16, %network.2.weight, %network.2.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %18 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%17) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %19 : Float(1, 28, 14, 14, strides=[5488, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%18) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:719:0\n",
      "  %20 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%19, %network.5.weight, %network.5.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %21 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%20) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %22 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%21, %network.7.weight, %network.7.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %23 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%22) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %24 : Float(1, 56, 7, 7, strides=[2744, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%23) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:719:0\n",
      "  %25 : Float(1, 2744, strides=[2744, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1](%24) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/flatten.py:42:0\n",
      "  %26 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%25, %network.11.weight, %network.11.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  %27 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%26) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %28 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%27, %network.13.weight, %network.13.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  %29 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%28) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %30 : Float(1, 26, strides=[26, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%29, %network.15.weight, %network.15.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  return (%30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save Pytorch Model\n",
    "FILE = \"/home/ayon_chakroborty/Desktop/Cardano DNN/model3.pth\"\n",
    "torch.save(model.state_dict(), FILE)\n",
    "\n",
    "onnx_model_path = \"/home/ayon_chakroborty/Desktop/Cardano DNN/CNNmodel_version_3.onnx\"\n",
    "x = torch.randn(1, 1, 28, 28, device='cuda') # Sample input in the shape that the model expects\n",
    "torch.onnx.export(model, x, onnx_model_path, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637ca86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
