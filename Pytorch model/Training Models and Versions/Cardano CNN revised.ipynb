{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925d224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52519f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "raw_train_data = pd.read_csv('./SL Dataset/sign_mnist_train/sign_mnist_train.csv', sep=\",\")\n",
    "raw_test_data = pd.read_csv('./SL Dataset/sign_mnist_test/sign_mnist_test.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46dcbfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 785)\n",
      "(7172, 785)\n",
      "\n",
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      3     107     118     127     134     139     143     146     150   \n",
      "1      6     155     157     156     156     156     157     156     158   \n",
      "2      2     187     188     188     187     187     186     187     188   \n",
      "3      2     211     211     212     212     211     210     211     210   \n",
      "4     13     164     167     170     172     176     179     180     184   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0     153  ...       207       207       207       207       206       206   \n",
      "1     158  ...        69       149       128        87        94       163   \n",
      "2     187  ...       202       201       200       199       198       199   \n",
      "3     210  ...       235       234       233       231       230       226   \n",
      "4     185  ...        92       105       105       108       133       163   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0       206       204       203       202  \n",
      "1       175       103       135       149  \n",
      "2       198       195       194       195  \n",
      "3       225       222       229       163  \n",
      "4       157       163       164       179  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_data.shape)\n",
    "print(raw_test_data.shape)\n",
    "print()\n",
    "print(raw_train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f545ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to show output as letters\n",
    "alphabet = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:\"F\", 6:'G',\n",
    "             7:'H', 8:'I', 9:'J', 10:'K', 11:'L', 12:'M', 13:'N',\n",
    "        14:'O', 15:'P', 16:'Q', 17:'R', 18:'S', 19:'T', 20:'U', 21:'V', \n",
    "             22:'W', 23:'X', 24:'Y', 25:'Z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D nummpy arrays\n",
    "def Data2NParray(train_data, test_data):\n",
    "    train = train_data.copy(deep = True)\n",
    "    test = test_data.copy(deep = True)\n",
    "    train_img = train.iloc[:, 1:].to_numpy(dtype = 'float32')\n",
    "    test_img = test.iloc[:, 1:].to_numpy(dtype = 'float32')\n",
    "    return train_img, test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2581ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, test_img = Data2NParray(raw_train_data, raw_test_data)\n",
    "train_labels = raw_train_data['label'].values\n",
    "test_labels = raw_test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104e1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21524720\n",
      "\n",
      "\n",
      "\n",
      "7172\n",
      "\n",
      "\n",
      "\n",
      "[ 3  6  2 ... 18 17 23]\n",
      "\n",
      "\n",
      "\n",
      "[ 6  5 10 ...  2  4  2]\n"
     ]
    }
   ],
   "source": [
    "print(train_img.size)\n",
    "print('\\n\\n')\n",
    "print(test_img.shape[0])\n",
    "print('\\n\\n')\n",
    "print(train_labels)\n",
    "print('\\n\\n')\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fbbedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images to 3d array of 1, 28, 28\n",
    "train_img = train_img.reshape(train_img.shape[0], 1, 28, 28)\n",
    "test_img = test_img.reshape(test_img.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Numpy arrays into tensors\n",
    "train_img_tensor = torch.from_numpy(train_img)\n",
    "test_img_tensor = torch.from_numpy(test_img)\n",
    "\n",
    "train_label_tensor = torch.from_numpy(train_labels)\n",
    "test_label_tensor = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f11c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pytorch Dataset\n",
    "training_Dataset = TensorDataset(train_img_tensor, train_label_tensor)\n",
    "test_Dataset = TensorDataset(test_img_tensor, test_label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56023bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27455 torch.Size([1, 28, 28]) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# Each image is now converted to a (1,28,28) tensor\n",
    "image, label = training_Dataset[0]\n",
    "print(len(training_Dataset), image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5dd4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) tensor(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[107., 118., 127., 134., 139., 143., 146., 150., 153., 156., 158.,\n",
       "          160., 163., 165., 159., 166., 168., 170., 170., 171., 171., 171.,\n",
       "          172., 171., 171., 170., 170., 169.],\n",
       "         [111., 121., 129., 135., 141., 144., 148., 151., 154., 157., 160.,\n",
       "          163., 164., 170., 119., 152., 171., 171., 170., 171., 172., 172.,\n",
       "          172., 172., 172., 171., 171., 170.],\n",
       "         [113., 123., 131., 137., 142., 145., 150., 152., 155., 158., 161.,\n",
       "          163., 164., 172., 105., 142., 170., 171., 171., 171., 172., 172.,\n",
       "          173., 173., 172., 171., 171., 171.],\n",
       "         [116., 125., 133., 139., 143., 146., 151., 153., 156., 159., 162.,\n",
       "          163., 167., 167.,  95., 144., 171., 172., 172., 172., 172., 172.,\n",
       "          173., 173., 173., 172., 172., 171.],\n",
       "         [117., 126., 134., 140., 145., 149., 153., 156., 158., 161., 163.,\n",
       "          164., 175., 156.,  87., 154., 172., 173., 173., 173., 173., 173.,\n",
       "          174., 174., 174., 173., 172., 172.],\n",
       "         [119., 128., 136., 142., 146., 150., 153., 156., 159., 163., 165.,\n",
       "          164., 184., 148.,  89., 164., 172., 174., 174., 174., 174., 175.,\n",
       "          175., 174., 175., 174., 173., 173.],\n",
       "         [122., 130., 138., 143., 147., 150., 154., 158., 162., 165., 166.,\n",
       "          172., 181., 128.,  94., 170., 173., 175., 174., 175., 176., 177.,\n",
       "          177., 177., 177., 175., 175., 174.],\n",
       "         [122., 132., 139., 145., 149., 152., 156., 160., 163., 165., 166.,\n",
       "          181., 172., 103., 113., 175., 176., 178., 178., 179., 179., 179.,\n",
       "          179., 178., 179., 177., 175., 174.],\n",
       "         [125., 134., 141., 147., 150., 153., 157., 161., 164., 167., 168.,\n",
       "          184., 179., 116., 126., 165., 176., 179., 180., 180., 181., 180.,\n",
       "          180., 180., 179., 178., 177., 176.],\n",
       "         [128., 135., 142., 148., 152., 154., 158., 162., 165., 168., 170.,\n",
       "          187., 180., 156., 161., 124., 143., 179., 178., 178., 181., 182.,\n",
       "          181., 180., 181., 180., 179., 179.],\n",
       "         [129., 136., 144., 150., 153., 155., 159., 163., 166., 169., 172.,\n",
       "          187., 184., 153., 102., 117., 110., 175., 169., 154., 182., 183.,\n",
       "          183., 182., 182., 181., 181., 179.],\n",
       "         [131., 138., 145., 150., 155., 157., 161., 165., 168., 174., 190.,\n",
       "          189., 175., 146.,  94.,  97., 113., 151., 158., 129., 184., 184.,\n",
       "          184., 184., 183., 183., 182., 180.],\n",
       "         [131., 139., 146., 151., 155., 159., 163., 167., 175., 182., 179.,\n",
       "          171., 159., 114., 102.,  89., 121., 136., 136.,  96., 172., 186.,\n",
       "          186., 185., 185., 184., 182., 181.],\n",
       "         [131., 140., 147., 154., 157., 160., 164., 179., 186., 191., 187.,\n",
       "          180., 157., 100.,  88.,  84., 108., 111., 126.,  90., 120., 186.,\n",
       "          187., 187., 186., 185., 184., 182.],\n",
       "         [133., 141., 149., 155., 158., 160., 174., 201., 189., 165., 151.,\n",
       "          143., 146., 120.,  87.,  78.,  87.,  76., 108.,  98.,  96., 181.,\n",
       "          188., 187., 186., 186., 185., 183.],\n",
       "         [133., 141., 150., 156., 160., 161., 179., 197., 174., 135.,  99.,\n",
       "           72.,  95., 134.,  97.,  72.,  74.,  68., 116., 105., 108., 187.,\n",
       "          189., 187., 187., 186., 186., 185.],\n",
       "         [134., 143., 151., 156., 161., 163., 179., 194., 156., 110.,  74.,\n",
       "           42.,  52., 139.,  94.,  67.,  75.,  75., 118., 106., 129., 189.,\n",
       "          191., 190., 188., 188., 187., 186.],\n",
       "         [135., 144., 152., 158., 163., 163., 177., 193., 161., 122.,  84.,\n",
       "           43.,  71., 134.,  81.,  57.,  71.,  88., 112.,  98., 157., 193.,\n",
       "          193., 192., 190., 190., 189., 188.],\n",
       "         [136., 144., 152., 158., 162., 163., 176., 192., 164., 128.,  98.,\n",
       "           62.,  60., 100.,  71.,  76.,  96., 101., 105.,  95., 174., 195.,\n",
       "          194., 194., 194., 193., 191., 190.],\n",
       "         [137., 145., 152., 159., 164., 165., 178., 191., 164., 135., 113.,\n",
       "           82.,  59.,  87.,  98., 111., 120., 108.,  97., 108., 190., 196.,\n",
       "          195., 195., 194., 193., 193., 192.],\n",
       "         [139., 146., 154., 160., 164., 165., 175., 186., 163., 139., 112.,\n",
       "           85.,  67., 102., 126., 133., 126., 105., 104., 176., 197., 198.,\n",
       "          197., 196., 195., 195., 194., 193.],\n",
       "         [138., 147., 155., 161., 165., 167., 172., 186., 163., 137., 107.,\n",
       "           87.,  76., 106., 122., 125., 117.,  96., 156., 199., 199., 200.,\n",
       "          198., 196., 196., 195., 195., 194.],\n",
       "         [139., 148., 156., 163., 166., 168., 172., 180., 158., 131., 108.,\n",
       "           99.,  86., 108., 118., 116., 103., 107., 191., 202., 201., 200.,\n",
       "          200., 200., 199., 197., 198., 196.],\n",
       "         [140., 149., 157., 164., 168., 167., 177., 178., 155., 131., 118.,\n",
       "          105.,  87., 100., 106., 100.,  96., 164., 202., 202., 202., 202.,\n",
       "          202., 201., 200., 199., 199., 198.],\n",
       "         [140., 150., 157., 165., 167., 170., 181., 175., 152., 130., 115.,\n",
       "           98.,  82.,  85.,  90.,  99., 165., 202., 203., 204., 203., 203.,\n",
       "          202., 202., 201., 201., 200., 200.],\n",
       "         [142., 150., 159., 165., 170., 191., 173., 157., 144., 119.,  97.,\n",
       "           84.,  79.,  79.,  91., 172., 202., 203., 203., 205., 204., 204.,\n",
       "          204., 203., 202., 202., 201., 200.],\n",
       "         [142., 151., 160., 165., 188., 190., 187., 150., 119., 109.,  85.,\n",
       "           79.,  79.,  78., 137., 203., 205., 206., 206., 207., 207., 206.,\n",
       "          206., 204., 205., 204., 203., 202.],\n",
       "         [142., 151., 160., 172., 196., 188., 188., 190., 135.,  96.,  86.,\n",
       "           77.,  77.,  79., 176., 205., 207., 207., 207., 207., 207., 207.,\n",
       "          206., 206., 206., 204., 203., 202.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = training_Dataset[0]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6225d407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7455, 20000, 7172)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create validation set and training set\n",
    "validation_sz = 7455\n",
    "training_sz = len(training_Dataset) - validation_sz\n",
    "\n",
    "training_set, validation_set = random_split(training_Dataset, [training_sz, validation_sz])\n",
    "\n",
    "len(validation_set), len(training_set), len(test_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d6cf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "batch_size = 60\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0356407",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 15\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation\n",
    "train_dl = DataLoader(training_set, batch_size, shuffle = True, num_workers=4, pin_memory=True)\n",
    "validation_dl = DataLoader(validation_set, batch_size*2, shuffle = True, num_workers=4, pin_memory=True)\n",
    "test_dl = DataLoader(test_Dataset, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ef456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Network class and make helper methods for training and validation\n",
    "class Network(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        # Generate Predictions\n",
    "        out = self(images)\n",
    "        # Calculate Loss\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        # Generate Predictions\n",
    "        out = self(images)\n",
    "        # Calculate Loss\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        # Calculate Accuracy\n",
    "        acc = accuracy(out, labels)\n",
    "        return{'val_loss': loss.detach(),'val_accuracy': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        # Combine Losses\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_acc = [x['val_accuracy'] for x in outputs]\n",
    "        # Combine Accuracies\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_accuracy': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train loss: {:.3f}, val loss {:.3f}, val acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_accuracy']))\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e867cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(Network):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 28, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(28, 28, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # Ouput image size: 28x14x14\n",
    "\n",
    "            nn.Conv2d(28, 56, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(56, 56, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # Ouput image size: 56x7x7\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(56*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56de5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for training Model\n",
    "input_ch = 1\n",
    "input_size = input_ch * 28 * 28\n",
    "classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba1481b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=2744, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=128, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel(input_ch, classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "febb6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def to_device(data, device):\n",
    "    # Move Tensors to a chosen device\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    # Move Data to the device\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "            \n",
    "    def __len(self):\n",
    "        # Number of batches\n",
    "        return len(self.dl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f02c5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3f30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(CNNModel(input_ch, classes), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d2371fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(validation_dl, device)\n",
    "test_dl = DeviceDataLoader(test_dl, device)\n",
    "\n",
    "print(train_dl.device)\n",
    "print(test_dl.device)\n",
    "print(val_dl.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62a6fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_dl):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55eb0af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 3.3297271728515625, 'val_accuracy': 0.034656085073947906}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8097c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train loss: 3.111, val loss 3.069, val acc: 0.1078\n",
      "Epoch [1], train loss: 2.224, val loss 2.219, val acc: 0.3351\n",
      "Epoch [2], train loss: 1.117, val loss 1.070, val acc: 0.6690\n",
      "Epoch [3], train loss: 0.570, val loss 0.499, val acc: 0.8200\n",
      "Epoch [4], train loss: 0.276, val loss 0.154, val acc: 0.9606\n",
      "Epoch [5], train loss: 0.126, val loss 0.343, val acc: 0.8987\n",
      "Epoch [6], train loss: 0.058, val loss 0.107, val acc: 0.9661\n",
      "Epoch [7], train loss: 0.048, val loss 0.022, val acc: 0.9989\n",
      "Epoch [8], train loss: 0.014, val loss 0.020, val acc: 0.9984\n",
      "Epoch [9], train loss: 0.009, val loss 0.008, val acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, learning_rate , model, train_dl, val_dl, opt_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34c86b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4900277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a079905c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.42141038179397583, 'val_accuracy': 0.8872584700584412}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with the tesing dataloader\n",
    "result = evaluate(model, test_dl)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836d7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9279cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(6) , Predicted: 6\n",
      "Label: tensor(5) , Predicted: 5\n",
      "Label: tensor(10) , Predicted: 10\n",
      "Label: tensor(0) , Predicted: 0\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(21) , Predicted: 22\n",
      "Label: tensor(10) , Predicted: 10\n",
      "Label: tensor(14) , Predicted: 14\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(7) , Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    img, label = test_Dataset[x]\n",
    "    print('Label:', label, ', Predicted:', predict_image(img, model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e826a53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"ayon-chakroborty/cardano-cnn-revised\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/ayon-chakroborty/cardano-cnn-revised\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ayon-chakroborty/cardano-cnn-revised'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d3d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Pytorch Model\n",
    "FILE = \"/home/ayon_chakroborty/Desktop/Cardano DNN/model2.pth\"\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba656515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=2744, out_features=512, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=128, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved Model\n",
    "loaded_model = CNNModel(input_ch, classes)\n",
    "loaded_model.load_state_dict(torch.load(FILE))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14932821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(6) , Predicted: 6\n",
      "Label: tensor(5) , Predicted: 5\n",
      "Label: tensor(10) , Predicted: 10\n",
      "Label: tensor(0) , Predicted: 0\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(21) , Predicted: 22\n",
      "Label: tensor(10) , Predicted: 10\n",
      "Label: tensor(14) , Predicted: 14\n",
      "Label: tensor(3) , Predicted: 3\n",
      "Label: tensor(7) , Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "# Test the loaded Model\n",
    "loaded_model = to_device(loaded_model, device)\n",
    "for x in range(10):\n",
    "    img, label = test_Dataset[x]\n",
    "    print('Label:', label, ', Predicted:', predict_image(img, loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa224591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.42141038179397583, 'val_accuracy': 0.8872584700584412}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the loaded Model with test dataset\n",
    "ld_result = evaluate(loaded_model, test_dl)\n",
    "ld_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4d91b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"ayon-chakroborty/cardano-cnn-revised\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/ayon-chakroborty/cardano-cnn-revised\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ayon-chakroborty/cardano-cnn-revised'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "413e627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %network.0.weight : Float(28, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.0.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.2.weight : Float(28, 28, 3, 3, strides=[252, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.2.bias : Float(28, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.5.weight : Float(56, 28, 3, 3, strides=[252, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.5.bias : Float(56, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.7.weight : Float(56, 56, 3, 3, strides=[504, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.7.bias : Float(56, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.11.weight : Float(512, 2744, strides=[2744, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.11.bias : Float(512, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.13.weight : Float(128, 512, strides=[512, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.13.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %network.15.weight : Float(26, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %network.15.bias : Float(26, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %15 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.1, %network.0.weight, %network.0.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %16 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%15) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %17 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%16, %network.2.weight, %network.2.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %18 : Float(1, 28, 28, 28, strides=[21952, 784, 28, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%17) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %19 : Float(1, 28, 14, 14, strides=[5488, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%18) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:719:0\n",
      "  %20 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%19, %network.5.weight, %network.5.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %21 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%20) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %22 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%21, %network.7.weight, %network.7.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/conv.py:442:0\n",
      "  %23 : Float(1, 56, 14, 14, strides=[10976, 196, 14, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%22) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %24 : Float(1, 56, 7, 7, strides=[2744, 49, 7, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%23) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:719:0\n",
      "  %25 : Float(1, 2744, strides=[2744, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1](%24) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/modules/flatten.py:42:0\n",
      "  %26 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%25, %network.11.weight, %network.11.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  %27 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%26) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %28 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%27, %network.13.weight, %network.13.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  %29 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%28) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1299:0\n",
      "  %30 : Float(1, 26, strides=[26, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%29, %network.15.weight, %network.15.bias) # /home/ayon_chakroborty/cv-env/lib/python3.9/site-packages/torch/nn/functional.py:1848:0\n",
      "  return (%30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"/home/ayon_chakroborty/Desktop/Cardano DNN/CNNmodel_version_2.onnx\"\n",
    "x = torch.randn(1, 1, 28, 28, device='cuda') # Sample input in the shape that the model expects\n",
    "torch.onnx.export(model, x, onnx_model_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
